{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Eva-4B-V2 Inference for EvasionBench\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IIIIQIIII/EvasionBench/blob/main/scripts/eva4b_inference.ipynb)\n\nThis notebook demonstrates how to use Eva-4B-V2 model to detect evasive answers in earnings call Q&A sessions.\n\n- **Model**: [FutureMa/Eva-4B-V2](https://huggingface.co/FutureMa/Eva-4B-V2)\n- **Dataset**: [FutureMa/EvasionBench](https://huggingface.co/datasets/FutureMa/EvasionBench)\n- **GitHub**: [IIIIQIIII/EvasionBench](https://github.com/IIIIQIIII/EvasionBench)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "PROMPT_TEMPLATE = \"\"\"You are a financial analyst. Your task is to Detect Evasive Answers in Financial Q&A\n\nQuestion: {question}\nAnswer: {answer}\n\nResponse format:\n```json\n{{\"label\": \"direct|intermediate|fully_evasive\"}}\n```\n\nAnswer in ```json content, no other text\"\"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FutureMa/Eva-4B-V2\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load EvasionBench Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"FutureMa/EvasionBench\", split=\"train\")\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "# Sample subset for demo\n",
    "NUM_SAMPLES = 5\n",
    "samples = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))\n",
    "print(f\"Selected {NUM_SAMPLES} samples for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_response(response: str) -> dict:\n    \"\"\"Parse JSON response from model output.\"\"\"\n    # Try to extract JSON from markdown code block\n    json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response, re.DOTALL)\n    if json_match:\n        try:\n            return json.loads(json_match.group(1))\n        except json.JSONDecodeError:\n            pass\n    \n    # Try to extract raw JSON\n    json_match = re.search(r'\\{[^{}]*\"label\"[^{}]*\\}', response)\n    if json_match:\n        try:\n            return json.loads(json_match.group(0))\n        except json.JSONDecodeError:\n            pass\n    \n    return {\"label\": \"unknown\"}\n\n\ndef predict(question: str, answer: str) -> tuple:\n    \"\"\"Run inference on a single Q&A pair.\"\"\"\n    prompt = PROMPT_TEMPLATE.format(question=question, answer=answer)\n    \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False\n    )\n    \n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=64,\n            temperature=0.1,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode only the generated tokens\n    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n    \n    return parse_response(response), response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(f\"Running inference on {len(samples)} samples\")\nprint(\"=\" * 60)\nprint()\n\ncorrect = 0\nresults = []\n\nfor i, sample in enumerate(samples):\n    uid = sample[\"uid\"]\n    question = sample[\"question\"]\n    answer = sample[\"answer\"]\n    gold_label = sample[\"eva4b_label\"]\n    \n    # Truncate for display\n    q_display = question[:100] + \"...\" if len(question) > 100 else question\n    a_display = answer[:100] + \"...\" if len(answer) > 100 else answer\n    \n    print(f\"Sample {i+1}/{len(samples)}\")\n    print(f\"  UID: {uid[:16]}...\")\n    print(f\"  Question: {q_display}\")\n    print(f\"  Answer: {a_display}\")\n    \n    result, raw_response = predict(question, answer)\n    pred_label = result.get(\"label\", \"unknown\")\n    \n    is_correct = pred_label == gold_label\n    correct += int(is_correct)\n    \n    status = \"✓\" if is_correct else \"✗\"\n    print(f\"  Gold: {gold_label} | Pred: {pred_label} {status}\")\n    print()\n    \n    results.append({\n        \"uid\": uid,\n        \"gold\": gold_label,\n        \"pred\": pred_label,\n        \"correct\": is_correct\n    })"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = correct / len(samples) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Results: {correct}/{len(samples)} correct ({accuracy:.1f}% accuracy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display results as table\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}