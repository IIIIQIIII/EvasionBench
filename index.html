<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EvasionBench: Detecting Managerial Evasion in Earnings Call Q&A</title>
    <link rel="icon" type="image/svg+xml" href="assets/favicon.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        /* Pastel Dreamland Adventure Color Palette */
        :root {
            --pastel-blue: #bde0fe;
            --pastel-pink-light: #ffc8dd;
            --pastel-pink: #ffafcc;
            --pastel-lavender: #cdb4db;
            --pastel-mint: #a8dadc;
            --text-dark: #2d3748;
            --text-gray: #4a5568;
            --text-light: #718096;
            --bg-white: #ffffff;
            --bg-light: #f8f9fa;
            --border-light: #e2e8f0;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            line-height: 1.7;
            color: var(--text-dark);
            background: var(--bg-white);
        }

        /* Header */
        .header {
            background: linear-gradient(135deg, var(--pastel-lavender) 0%, var(--pastel-pink-light) 50%, var(--pastel-blue) 100%);
            padding: 80px 20px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.8rem;
            font-weight: 700;
            color: var(--text-dark);
            margin-bottom: 16px;
        }

        .header .subtitle {
            font-size: 1.25rem;
            color: var(--text-gray);
            max-width: 800px;
            margin: 0 auto 30px;
        }

        .badges {
            display: flex;
            justify-content: center;
            gap: 12px;
            flex-wrap: wrap;
            margin-bottom: 30px;
        }

        .badge {
            display: inline-flex;
            align-items: center;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.9rem;
            font-weight: 500;
            text-decoration: none;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .badge:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }

        .badge-paper { background: var(--pastel-lavender); color: var(--text-dark); }
        .badge-github { background: var(--pastel-blue); color: var(--text-dark); }
        .badge-huggingface { background: var(--pastel-pink-light); color: var(--text-dark); }
        .badge-data { background: var(--pastel-mint); color: var(--text-dark); }

        /* Container */
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Section */
        .section {
            padding: 60px 0;
        }

        .section:nth-child(even) {
            background: var(--bg-light);
        }

        .section-title {
            font-size: 1.8rem;
            font-weight: 600;
            color: var(--text-dark);
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 3px solid var(--pastel-lavender);
            display: inline-block;
        }

        /* Abstract */
        .abstract {
            background: linear-gradient(135deg, rgba(205, 180, 219, 0.1) 0%, rgba(255, 200, 221, 0.1) 100%);
            border-left: 4px solid var(--pastel-lavender);
            padding: 24px 30px;
            border-radius: 0 12px 12px 0;
            font-size: 1.05rem;
            color: var(--text-gray);
        }

        /* Stats Grid */
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .stat-card {
            background: var(--bg-white);
            border-radius: 12px;
            padding: 24px;
            text-align: center;
            box-shadow: 0 2px 8px rgba(0,0,0,0.06);
            border: 1px solid var(--border-light);
            transition: transform 0.2s;
        }

        .stat-card:hover {
            transform: translateY(-4px);
        }

        .stat-card:nth-child(1) { border-top: 4px solid var(--pastel-blue); }
        .stat-card:nth-child(2) { border-top: 4px solid var(--pastel-pink-light); }
        .stat-card:nth-child(3) { border-top: 4px solid var(--pastel-pink); }
        .stat-card:nth-child(4) { border-top: 4px solid var(--pastel-lavender); }

        .stat-value {
            font-size: 2.2rem;
            font-weight: 700;
            color: var(--text-dark);
        }

        .stat-label {
            font-size: 0.95rem;
            color: var(--text-light);
            margin-top: 8px;
        }

        /* Figure */
        .figure {
            margin: 40px 0;
            text-align: center;
        }

        .figure img, .figure object {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        }

        .figure-caption {
            margin-top: 16px;
            font-size: 0.95rem;
            color: var(--text-light);
            font-style: italic;
        }

        /* Table - Clean Minimal Style */
        .table-wrapper {
            overflow-x: auto;
            margin: 30px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            background: transparent;
            font-size: 0.95rem;
        }

        th, td {
            padding: 12px 16px;
            text-align: left;
            border: none;
        }

        th {
            background: transparent;
            font-weight: 600;
            color: var(--text-dark);
            border-bottom: 2px solid var(--text-dark);
        }

        tbody tr {
            border-bottom: 1px solid #e0e0e0;
        }

        tbody tr:last-child {
            border-bottom: none;
        }

        tr:hover {
            background: transparent;
        }

        .highlight-row {
            background: rgba(205, 180, 219, 0.25);
        }

        .highlight-row:hover {
            background: rgba(205, 180, 219, 0.35);
        }

        td:first-child, th:first-child {
            color: var(--text-light);
        }

        td:last-child {
            font-weight: 500;
        }

        /* Taxonomy Cards */
        .taxonomy-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 24px;
            margin: 30px 0;
        }

        .taxonomy-card {
            background: var(--bg-white);
            border-radius: 12px;
            padding: 28px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.06);
            border: 1px solid var(--border-light);
        }

        .taxonomy-card.direct { border-left: 5px solid var(--pastel-mint); }
        .taxonomy-card.intermediate { border-left: 5px solid var(--pastel-pink); }
        .taxonomy-card.evasive { border-left: 5px solid var(--pastel-lavender); }

        .taxonomy-card h3 {
            font-size: 1.2rem;
            margin-bottom: 12px;
            color: var(--text-dark);
        }

        .taxonomy-card p {
            color: var(--text-gray);
            font-size: 0.95rem;
        }

        .taxonomy-example {
            margin-top: 16px;
            padding: 12px;
            background: var(--bg-light);
            border-radius: 8px;
            font-size: 0.9rem;
            color: var(--text-gray);
        }

        /* Two Column */
        .two-column {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(450px, 1fr));
            gap: 40px;
            align-items: start;
        }

        /* Footer */
        .footer {
            background: var(--text-dark);
            color: var(--bg-light);
            padding: 40px 20px;
            text-align: center;
        }

        .footer a {
            color: var(--pastel-pink-light);
            text-decoration: none;
        }

        .footer a:hover {
            text-decoration: underline;
        }

        /* BibTeX */
        .bibtex {
            background: #1a202c;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Monaco', 'Consolas', monospace;
            font-size: 0.85rem;
            overflow-x: auto;
            text-align: left;
            margin-top: 20px;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .header h1 {
                font-size: 2rem;
            }
            .two-column {
                grid-template-columns: 1fr;
            }
            .stats-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }
    </style>
</head>
<body>

    <!-- Header -->
    <header class="header">
        <h1>EvasionBench</h1>
        <p class="subtitle">A Large-Scale Benchmark for Detecting Managerial Evasion in Earnings Call Q&A</p>

        <div class="badges">
            <a href="#" class="badge badge-paper">üìÑ Paper (arXiv)</a>
            <a href="https://github.com/IIIIQIIII/EvasionBench" class="badge badge-github">üíª GitHub</a>
            <a href="https://huggingface.co/FutureMa/Eva-4B" class="badge badge-huggingface">ü§ó Model</a>
            <a href="#" class="badge badge-data">üìä Dataset</a>
        </div>
    </header>

    <!-- Abstract -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract">
                <p>
                    We introduce <strong>EvasionBench</strong>, a large-scale benchmark for detecting managerial evasion in earnings call Q&A sessions.
                    Built from 22.7 million Q&A pairs from S&P Capital IQ, we develop a three-level evasion taxonomy (<em>direct</em>, <em>intermediate</em>, <em>fully evasive</em>)
                    and a Multi-Model Consensus (MMC) annotation framework using frontier LLMs. Our benchmark includes 84K balanced training samples
                    and a 1K gold-standard evaluation set with human validation (Cohen's Œ∫ = 0.835). We introduce <strong>Eva-4B</strong>, a fine-tuned
                    Qwen3-4B model that achieves 84.9% Macro-F1, outperforming larger frontier models including Claude Opus 4.5 and GPT-5.2.
                </p>
            </div>
        </div>
    </section>

    <!-- Key Statistics -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Key Statistics</h2>
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-value">22.7M</div>
                    <div class="stat-label">Raw Q&A Pairs</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">84K</div>
                    <div class="stat-label">Training Samples</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">1K</div>
                    <div class="stat-label">Gold Evaluation Set</div>
                </div>
                <div class="stat-card">
                    <div class="stat-value">84.9%</div>
                    <div class="stat-label">Eva-4B Macro-F1</div>
                </div>
            </div>
        </div>
    </section>

    <!-- Evasion Taxonomy -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Evasion Taxonomy</h2>
            <div class="taxonomy-grid">
                <div class="taxonomy-card direct">
                    <h3>‚úÖ Direct</h3>
                    <p>The core question is directly and explicitly answered with clear figures, "Yes/No" stance, or direct explanations.</p>
                    <div class="taxonomy-example">
                        <strong>Q:</strong> "What is the expected margin for Q4?"<br>
                        <strong>A:</strong> "We expect it to be 32%."
                    </div>
                </div>
                <div class="taxonomy-card intermediate">
                    <h3>‚ö†Ô∏è Intermediate</h3>
                    <p>The response provides related context but sidesteps the specific core through hedging or answering adjacent topics.</p>
                    <div class="taxonomy-example">
                        <strong>Q:</strong> "What is the expected margin for Q4?"<br>
                        <strong>A:</strong> "We expect margins to improve relative to Q3."
                    </div>
                </div>
                <div class="taxonomy-card evasive">
                    <h3>‚ùå Fully Evasive</h3>
                    <p>The question is ignored, explicitly refused, or the response is entirely off-topic with no relevant information.</p>
                    <div class="taxonomy-example">
                        <strong>Q:</strong> "What is the expected margin for Q4?"<br>
                        <strong>A:</strong> "We are focused on driving long-term shareholder value."
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- MMC Pipeline -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Multi-Model Consensus (MMC) Framework</h2>
            <p style="color: var(--text-gray); margin-bottom: 20px;">
                Our MMC framework leverages multiple frontier LLMs for annotation, with a three-judge majority voting mechanism to resolve disagreements.
            </p>
            <div class="figure">
                <object type="image/svg+xml" data="assets/mmc_pipe.svg" style="width: 100%; max-width: 900px;"></object>
                <p class="figure-caption">Figure 1: The Multi-Model Consensus (MMC) annotation pipeline.</p>
            </div>
        </div>
    </section>

    <!-- Model Performance (Top 5 Chart) -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Model Performance</h2>
            <p style="color: var(--text-gray); margin-bottom: 20px;">
                Top 5 models on EvasionBench 1K evaluation set. <strong>Eva-4B (Full)</strong> achieves the highest Macro-F1, outperforming frontier LLMs including Gemini 3 Flash and Claude Opus 4.5.
            </p>
            <div class="figure">
                <object type="image/svg+xml" data="assets/top5_performance.svg" style="width: 100%; max-width: 1200px;"></object>
                <p class="figure-caption">Figure: Top 5 model performance comparison (Macro-F1 %).</p>
            </div>
        </div>
    </section>

    <!-- Leaderboard (Full Table) -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Leaderboard</h2>
            <div class="table-wrapper">
                <table>
                    <thead>
                        <tr>
                            <th>Rank</th>
                            <th>Model</th>
                            <th>Category</th>
                            <th>Accuracy</th>
                            <th>Macro-F1</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="highlight-row">
                            <td>1</td>
                            <td><strong>Eva-4B (Full)</strong></td>
                            <td>Eva-4B</td>
                            <td>84.8%</td>
                            <td><strong>84.9%</strong></td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>Gemini 3 Flash</td>
                            <td>Closed-Source</td>
                            <td>84.6%</td>
                            <td>84.64%</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>Claude Opus 4.5</td>
                            <td>Closed-Source</td>
                            <td>84.1%</td>
                            <td>84.38%</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>GLM-4.7</td>
                            <td>Open-Source</td>
                            <td>83.1%</td>
                            <td>82.91%</td>
                        </tr>
                        <tr>
                            <td>5</td>
                            <td>Eva-4B (Consensus)</td>
                            <td>Eva-4B</td>
                            <td>81.0%</td>
                            <td>81.37%</td>
                        </tr>
                        <tr>
                            <td>6</td>
                            <td>GPT-5.2</td>
                            <td>Closed-Source</td>
                            <td>80.8%</td>
                            <td>80.90%</td>
                        </tr>
                        <tr>
                            <td>7</td>
                            <td>Eva-4B (Opus Only)</td>
                            <td>Eva-4B</td>
                            <td>80.6%</td>
                            <td>80.61%</td>
                        </tr>
                        <tr>
                            <td>8</td>
                            <td>Qwen3-Coder</td>
                            <td>Open-Source</td>
                            <td>78.0%</td>
                            <td>78.16%</td>
                        </tr>
                        <tr>
                            <td>9</td>
                            <td>MiniMax-M2.1</td>
                            <td>Open-Source</td>
                            <td>71.8%</td>
                            <td>71.31%</td>
                        </tr>
                        <tr>
                            <td>10</td>
                            <td>DeepSeek-V3.2</td>
                            <td>Open-Source</td>
                            <td>66.7%</td>
                            <td>66.88%</td>
                        </tr>
                        <tr>
                            <td>11</td>
                            <td>Kimi-K2</td>
                            <td>Open-Source</td>
                            <td>67.8%</td>
                            <td>66.68%</td>
                        </tr>
                        <tr>
                            <td>12</td>
                            <td>Qwen3-4B (Base)</td>
                            <td>Base Model</td>
                            <td>42.3%</td>
                            <td>34.30%</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <!-- Training & Ablation -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Eva-4B Training & Ablation Study</h2>
            <div class="two-column">
                <div class="figure">
                    <object type="image/svg+xml" data="assets/training_pipeline_pastel.svg" style="width: 100%;"></object>
                    <p class="figure-caption">Figure 2: Two-stage training pipeline for Eva-4B.</p>
                </div>
                <div class="figure">
                    <object type="image/svg+xml" data="assets/eva_4b_ablation_study_pastel.svg" style="width: 100%;"></object>
                    <p class="figure-caption">Figure 3: Ablation study comparing model variants.</p>
                </div>
            </div>
            <div class="two-column" style="margin-top: 40px;">
                <div class="figure">
                    <img src="assets/training_loss_curve_pastel.png" alt="Training Loss Curve">
                    <p class="figure-caption">Figure 4: Training loss curves for two-stage fine-tuning.</p>
                </div>
                <div class="figure">
                    <img src="assets/eva_4b_confusion_matrix_pastel.png" alt="Confusion Matrix">
                    <p class="figure-caption">Figure 5: Eva-4B (Full) confusion matrix on 1K evaluation set.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Judge Analysis -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">LLM Judge Analysis</h2>
            <div class="two-column">
                <div class="figure">
                    <img src="assets/judge_label_distribution_pastel.png" alt="Judge Label Distribution">
                    <p class="figure-caption">Figure 6: Label distribution across three LLM judges.</p>
                </div>
                <div class="figure">
                    <img src="assets/position_bias_analysis_pastel.png" alt="Position Bias Analysis">
                    <p class="figure-caption">Figure 7: Position bias analysis in LLM-as-judge settings.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Citation -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Citation</h2>
            <p style="color: var(--text-gray); margin-bottom: 16px;">If you find EvasionBench useful, please cite our paper:</p>
            <div class="bibtex">
@article{evasionbench2026,
  title={EvasionBench: A Large-Scale Benchmark for Detecting Managerial Evasion in Earnings Call Q&A},
  author={...},
  journal={arXiv preprint arXiv:2602.xxxxx},
  year={2026}
}
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <p>EvasionBench ¬© 2026 | <a href="https://github.com/IIIIQIIII/EvasionBench">GitHub</a> | <a href="https://huggingface.co/FutureMa/Eva-4B">HuggingFace</a></p>
    </footer>

</body>
</html>
